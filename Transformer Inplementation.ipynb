{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.568227Z","iopub.execute_input":"2024-11-26T17:42:26.568810Z","iopub.status.idle":"2024-11-26T17:42:26.577230Z","shell.execute_reply.started":"2024-11-26T17:42:26.568769Z","shell.execute_reply":"2024-11-26T17:42:26.575490Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch\nimport math\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.579416Z","iopub.execute_input":"2024-11-26T17:42:26.579913Z","iopub.status.idle":"2024-11-26T17:42:26.592496Z","shell.execute_reply.started":"2024-11-26T17:42:26.579859Z","shell.execute_reply":"2024-11-26T17:42:26.591026Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"class InputEmbedding(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, \n                                      d_model)\n        self.scale = math.sqrt(d_model)  # Precompute scaling factor\n\n    def forward(self, x):\n        return self.embedding(x) * self.scale","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.594461Z","iopub.execute_input":"2024-11-26T17:42:26.594975Z","iopub.status.idle":"2024-11-26T17:42:26.611346Z","shell.execute_reply.started":"2024-11-26T17:42:26.594921Z","shell.execute_reply":"2024-11-26T17:42:26.609861Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class PositionEncoding(nn.Module):\n    def __init__(self,d_embed_model,max_seq_length,drop_out = 0.1):\n        super().__init__()\n        self.embed_dim = d_embed_model\n        self.max_seq = max_seq_length\n        self.dropout = nn.Dropout(drop_out)\n        pe = torch.zeros(max_seq_length,d_embed_model)\n        for pos in range(max_seq_length):\n            for i in range(0,d_embed_model,2):\n                pe[pos,i] = math.sin(pos / (10000 ** ((2 * (i)) / self.embed_dim)))\n                if i + 1 < d_embed_model:  \n                    pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / self.embed_dim)))\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe',pe)\n    def forward(self,x):\n        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.614025Z","iopub.execute_input":"2024-11-26T17:42:26.615394Z","iopub.status.idle":"2024-11-26T17:42:26.629969Z","shell.execute_reply.started":"2024-11-26T17:42:26.615301Z","shell.execute_reply":"2024-11-26T17:42:26.627972Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"class TransformerInputEmbedding(nn.Module):\n    def __init__(self,d_model,vocab_size,max_seq_len,dropout = 0.1):\n        super().__init__()\n        self.input_embedding = InputEmbedding(d_model,vocab_size)\n        self.position_encoding = PositionEncoding(d_model,max_seq_len,dropout)\n    def forward(self,x):\n        x = self.input_embedding(x)\n        x = self.position_encoding(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.632062Z","iopub.execute_input":"2024-11-26T17:42:26.632615Z","iopub.status.idle":"2024-11-26T17:42:26.652876Z","shell.execute_reply.started":"2024-11-26T17:42:26.632561Z","shell.execute_reply":"2024-11-26T17:42:26.651133Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, dropout):\n        super().__init__()\n        assert d_model % num_heads == 0, \\\n            \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        # Linear projections for Query, Key, Value\n        self.W_query = nn.Linear(d_model, d_model, bias=False)\n        self.W_key = nn.Linear(d_model, d_model, bias=False)\n        self.W_value = nn.Linear(d_model, d_model, bias=False)\n        self.out_prj = nn.Linear(d_model, d_model, bias=False)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value):\n        batch_size, seq_len, d_in = query.shape\n        \n        # Linear projections\n        queries = self.W_query(query)  # Shape: [batch_size, seq_len, d_model]\n        keys = self.W_key(key)\n        values = self.W_value(value)\n        \n        # Reshape for multi-head: [batch_size, seq_len, num_heads, head_dim]\n        queries = queries.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        keys = keys.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        values = values.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Attention scores: [batch_size, num_heads, seq_len, seq_len]\n        attn_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n        \n        # Attention weights\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        # Compute context vector: [batch_size, num_heads, seq_len, head_dim]\n        context_vec = torch.matmul(attn_weights, values)\n        \n        # Combine heads: [batch_size, seq_len, d_model]\n        context_vec = context_vec.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n        \n        # Final linear projection\n        output = self.out_prj(context_vec)\n        return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.654974Z","iopub.execute_input":"2024-11-26T17:42:26.655444Z","iopub.status.idle":"2024-11-26T17:42:26.672070Z","shell.execute_reply.started":"2024-11-26T17:42:26.655358Z","shell.execute_reply":"2024-11-26T17:42:26.670224Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self,emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self,x):\n        mean = x.mean(dim = -1,keepdim = True)\n        var = x.var(dim = -1,keepdim = True,unbiased = False)\n        norm_x = (x-mean)/torch.sqrt(var+self.eps)\n        return self.scale*norm_x+self.shift","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.685866Z","iopub.execute_input":"2024-11-26T17:42:26.686547Z","iopub.status.idle":"2024-11-26T17:42:26.696594Z","shell.execute_reply.started":"2024-11-26T17:42:26.686492Z","shell.execute_reply":"2024-11-26T17:42:26.695101Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self,x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715 * torch.pow(x, 3))\n        ))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.698983Z","iopub.execute_input":"2024-11-26T17:42:26.699572Z","iopub.status.idle":"2024-11-26T17:42:26.711007Z","shell.execute_reply.started":"2024-11-26T17:42:26.699515Z","shell.execute_reply":"2024-11-26T17:42:26.709305Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self,d_model,dropout):\n        super().__init__()\n        self.d_model = d_model\n        self.layers = nn.Sequential(\n            nn.Linear(self.d_model,4*self.d_model),\n            GELU(),\n            nn.Linear(4*self.d_model,self.d_model)\n        )\n    def forward(self,x):\n        return self.layers(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.712292Z","iopub.execute_input":"2024-11-26T17:42:26.712790Z","iopub.status.idle":"2024-11-26T17:42:26.728234Z","shell.execute_reply.started":"2024-11-26T17:42:26.712731Z","shell.execute_reply":"2024-11-26T17:42:26.726910Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"class ResidualConnection(nn.Module):\n    def __init__(self,d_model,dropout):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.norm = LayerNormalization(d_model)\n    def forward(self,x,sublayer):\n        return x + self.dropout(sublayer(self.norm(x)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.729922Z","iopub.execute_input":"2024-11-26T17:42:26.730436Z","iopub.status.idle":"2024-11-26T17:42:26.743561Z","shell.execute_reply.started":"2024-11-26T17:42:26.730383Z","shell.execute_reply":"2024-11-26T17:42:26.741889Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"class EncoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, dropout):\n        super().__init__()\n        self.encoder_attention = MultiheadAttention(d_model, num_heads, dropout)\n        self.residual1 = ResidualConnection(d_model, dropout)\n        self.feed_forward = FeedForward(d_model, dropout)\n        self.residual2 = ResidualConnection(d_model, dropout)\n        self.layer_norm1 = LayerNormalization(d_model)\n        self.layer_norm2 = LayerNormalization(d_model)\n\n    def forward(self, x):\n        # Pass query, key, and value explicitly for self-attention\n        x = self.residual1(x, lambda x: self.encoder_attention(x, x, x))  # Pass query, key, value\n        x = self.layer_norm1(x)\n\n        x = self.residual2(x, lambda x: self.feed_forward(x))  # Use lambda\n        x = self.layer_norm2(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.747560Z","iopub.execute_input":"2024-11-26T17:42:26.748128Z","iopub.status.idle":"2024-11-26T17:42:26.761256Z","shell.execute_reply.started":"2024-11-26T17:42:26.748071Z","shell.execute_reply":"2024-11-26T17:42:26.759907Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"class TransformerEncoder(nn.Module):\n    def __init__(self,d_model,num_heads,dropout,num_layers):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderBlock(d_model,num_heads,dropout)\n            for _ in range(num_layers)\n        ])\n        self.norm = LayerNormalization(d_model)\n    def forward(self,x):\n        for layer in self.layers:\n            x = layer(x)\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.763070Z","iopub.execute_input":"2024-11-26T17:42:26.763726Z","iopub.status.idle":"2024-11-26T17:42:26.778229Z","shell.execute_reply.started":"2024-11-26T17:42:26.763669Z","shell.execute_reply":"2024-11-26T17:42:26.776220Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"class MaskedMultiheadAttention(nn.Module):\n    def __init__(self,d_model,num_heads,dropout,context_length):\n        super().__init__()\n        assert d_model % num_heads == 0, \\\n            \"d_model must be divisible by num_heads\"\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n         # Linear projections for Query, Key, Value\n        self.W_query = nn.Linear(d_model, d_model, bias=False)\n        self.W_key = nn.Linear(d_model, d_model, bias=False)\n        self.W_value = nn.Linear(d_model, d_model, bias=False)\n        self.out_prj = nn.Linear(d_model, d_model, bias=False)\n\n        # Dropout for regularization\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer(\n            \"mask\",\n            torch.triu(torch.ones(context_length, context_length),\n                       diagonal=1)\n        )\n\n    def forward(self,x):\n        b, num_tokens, d_in = x.shape\n\n        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n\n        # We implicitly split the matrix by adding a `num_heads` dimension\n        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n        keys = keys.transpose(1, 2)\n        queries = queries.transpose(1, 2)\n        values = values.transpose(1, 2)\n\n        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n\n        # Original mask truncated to the number of tokens and converted to boolean\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n\n        # Use the mask to fill attention scores\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n        \n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n\n        # Shape: (b, num_tokens, num_heads, head_dim)\n        context_vec = (attn_weights @ values).transpose(1, 2) \n        \n        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_model)\n        context_vec = self.out_prj(context_vec) # optional projection\n\n        return context_vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.780188Z","iopub.execute_input":"2024-11-26T17:42:26.780741Z","iopub.status.idle":"2024-11-26T17:42:26.800243Z","shell.execute_reply.started":"2024-11-26T17:42:26.780696Z","shell.execute_reply":"2024-11-26T17:42:26.798752Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"class DecoderBlock(nn.Module):\n    def __init__(self, d_model, num_heads, dropout, context_length):\n        super().__init__()\n        self.masked_attention = MaskedMultiheadAttention(d_model, num_heads, dropout, context_length)\n        self.encoder_attention = MultiheadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, dropout)\n        self.residual1 = ResidualConnection(d_model, dropout)\n        self.residual2 = ResidualConnection(d_model, dropout)\n        self.residual3 = ResidualConnection(d_model, dropout)\n        self.layernorm1 = LayerNormalization(d_model)\n        self.layernorm2 = LayerNormalization(d_model)\n        self.layernorm3 = LayerNormalization(d_model)\n\n    def forward(self, x, encoder_output):\n        # Masked Self Attention\n        x = self.residual1(x, lambda x: self.masked_attention(x))  # Use lambda\n        x = self.layernorm1(x)\n\n        # Encoder-Decoder Attention\n        x = self.residual2(x, lambda x: self.encoder_attention(x, encoder_output, encoder_output))  # Pass query, key, value\n        x = self.layernorm2(x)\n\n        # Feed Forward\n        x = self.residual3(x, lambda x: self.feed_forward(x))  # Use lambda\n        x = self.layernorm3(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.802025Z","iopub.execute_input":"2024-11-26T17:42:26.802725Z","iopub.status.idle":"2024-11-26T17:42:26.825258Z","shell.execute_reply.started":"2024-11-26T17:42:26.802661Z","shell.execute_reply":"2024-11-26T17:42:26.823501Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"class TransformerDecoder(nn.Module):\n    def __init__(self,d_model,num_heads,dropout,context_length,num_layers):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            DecoderBlock(d_model,num_heads,dropout,context_length)\n            for _ in range(num_layers)\n        ])\n        self.norm = LayerNormalization(d_model)\n    def forward(self,x,encoder_output):\n        for layer in self.layers:\n            x = layer(x,encoder_output)\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.826908Z","iopub.execute_input":"2024-11-26T17:42:26.827447Z","iopub.status.idle":"2024-11-26T17:42:26.848049Z","shell.execute_reply.started":"2024-11-26T17:42:26.827394Z","shell.execute_reply":"2024-11-26T17:42:26.846452Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"class ProjectionLayer(nn.Module):\n    def __init__(self,d_model,vocab_size):\n        super().__init__()\n        self.proj = nn.Linear(d_model,vocab_size)\n    def forward(self,x):\n        return self.proj(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.849827Z","iopub.execute_input":"2024-11-26T17:42:26.850626Z","iopub.status.idle":"2024-11-26T17:42:26.860910Z","shell.execute_reply.started":"2024-11-26T17:42:26.850573Z","shell.execute_reply":"2024-11-26T17:42:26.859586Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self,src_vocab_size,tgt_vocab_size,d_model,num_heads,\n                 dropout,num_encoder_layers,num_decoder_layers,max_seq_len):\n        super().__init__()\n        self.src_embedding = TransformerInputEmbedding(d_model,src_vocab_size,\n                                                      max_seq_len,dropout)\n        self.tgt_embedding = TransformerInputEmbedding(d_model,tgt_vocab_size,\n                                                      max_seq_len,dropout)\n        self.encoder = TransformerEncoder(d_model,num_heads,dropout,\n                                          num_encoder_layers)\n        self.decoder = TransformerDecoder(d_model,num_heads,dropout,\n                                          max_seq_len,\n                                          num_decoder_layers)\n        self.fc_out = ProjectionLayer(d_model,tgt_vocab_size)\n    def forward(self,src,tgt,apply_softmax = False):\n        # Embedding + Positional Encoding for source\n        src = self.src_embedding(src)\n        # Embedding + Positional Encoding for target\n        tgt = self.tgt_embedding(tgt)\n\n        encoder_output = self.encoder(src)\n        decoder_output = self.decoder(tgt,encoder_output)\n        output = self.fc_out(decoder_output)\n        if apply_softmax:\n            output = nn.functional.softmax(output, dim=-1)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:42:26.862925Z","iopub.execute_input":"2024-11-26T17:42:26.863416Z","iopub.status.idle":"2024-11-26T17:42:26.881199Z","shell.execute_reply.started":"2024-11-26T17:42:26.863348Z","shell.execute_reply":"2024-11-26T17:42:26.879980Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"src_vocab_size = 11\ntarget_vocab_size = 11\nnum_layers = 6\nseq_length= 12\nsrc = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\ntarget = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\nmodel = Transformer(src_vocab_size = src_vocab_size,\n                    tgt_vocab_size = target_vocab_size,\n                    d_model = 512, num_heads = 8,\n                    dropout = 0.1, \n                    num_encoder_layers = num_layers,\n                    num_decoder_layers = num_layers,\n                    max_seq_len = seq_length)\noutput = model(src, target)\nprint(output.shape) \nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T17:43:49.265877Z","iopub.execute_input":"2024-11-26T17:43:49.266315Z","iopub.status.idle":"2024-11-26T17:43:49.804673Z","shell.execute_reply.started":"2024-11-26T17:43:49.266274Z","shell.execute_reply":"2024-11-26T17:43:49.803548Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 12, 11])\nTransformer(\n  (src_embedding): TransformerInputEmbedding(\n    (input_embedding): InputEmbedding(\n      (embedding): Embedding(11, 512)\n    )\n    (position_encoding): PositionEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (tgt_embedding): TransformerInputEmbedding(\n    (input_embedding): InputEmbedding(\n      (embedding): Embedding(11, 512)\n    )\n    (position_encoding): PositionEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-5): 6 x EncoderBlock(\n        (encoder_attention): MultiheadAttention(\n          (W_query): Linear(in_features=512, out_features=512, bias=False)\n          (W_key): Linear(in_features=512, out_features=512, bias=False)\n          (W_value): Linear(in_features=512, out_features=512, bias=False)\n          (out_prj): Linear(in_features=512, out_features=512, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (residual1): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNormalization()\n        )\n        (feed_forward): FeedForward(\n          (layers): Sequential(\n            (0): Linear(in_features=512, out_features=2048, bias=True)\n            (1): GELU()\n            (2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n        (residual2): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNormalization()\n        )\n        (layer_norm1): LayerNormalization()\n        (layer_norm2): LayerNormalization()\n      )\n    )\n    (norm): LayerNormalization()\n  )\n  (decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-5): 6 x DecoderBlock(\n        (masked_attention): MaskedMultiheadAttention(\n          (W_query): Linear(in_features=512, out_features=512, bias=False)\n          (W_key): Linear(in_features=512, out_features=512, bias=False)\n          (W_value): Linear(in_features=512, out_features=512, bias=False)\n          (out_prj): Linear(in_features=512, out_features=512, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder_attention): MultiheadAttention(\n          (W_query): Linear(in_features=512, out_features=512, bias=False)\n          (W_key): Linear(in_features=512, out_features=512, bias=False)\n          (W_value): Linear(in_features=512, out_features=512, bias=False)\n          (out_prj): Linear(in_features=512, out_features=512, bias=False)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (feed_forward): FeedForward(\n          (layers): Sequential(\n            (0): Linear(in_features=512, out_features=2048, bias=True)\n            (1): GELU()\n            (2): Linear(in_features=2048, out_features=512, bias=True)\n          )\n        )\n        (residual1): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNormalization()\n        )\n        (residual2): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNormalization()\n        )\n        (residual3): ResidualConnection(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (norm): LayerNormalization()\n        )\n        (layernorm1): LayerNormalization()\n        (layernorm2): LayerNormalization()\n        (layernorm3): LayerNormalization()\n      )\n    )\n    (norm): LayerNormalization()\n  )\n  (fc_out): ProjectionLayer(\n    (proj): Linear(in_features=512, out_features=11, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":59}]}