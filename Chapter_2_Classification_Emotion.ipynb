{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511d8268",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e809565",
   "metadata": {},
   "source": [
    " Text classification is one of the most common tasks in NLP; it can be used for a broad range of applications, such as tagging customer feedback into categories or routing support tickets according to their language. Chances are that your email program's spam filter is using text classification to protect your inbox from a deluge of unwanted junk!"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db39f3f9",
   "metadata": {},
   "source": [
    "Now imagine that you are a data scientist who needs to build a system that can automatically identify emotional states such as \"anger\" or \"joy\" that people express about your company's product on Twitter. In this chapter, we'll tackle this task using a variant of BERT called DistilBERT.footnote:[V. Sanh et al., \"DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter\", (2019).] The main advantage of this model is that it achieves comparable performance to BERT, while being significantly smaller and more efficient. This enables us to train a classifier in a few minutes, and if you want to train a larger BERT model you can simply change the checkpoint of the pretrained model. A checkpoint corresponds to the set of weights that are loaded into a given transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35881bfc",
   "metadata": {},
   "source": [
    "# A First Look at Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55674527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 137494 data sets currently available on Hugging Face\n",
      "The first of ten are:['acronym_identification', 'ade_corpus_v2', 'UCLNLP/adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'allenai/ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "from datasets import list_datasets\n",
    "all_datasets = list_datasets()\n",
    "print(f\"There are {len(all_datasets)} data sets currently available on Hugging Face\")\n",
    "print(f\"The first of ten are:{all_datasets[:10]}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c7a4f03",
   "metadata": {},
   "source": [
    "Each dataset is given by a name, so we will load the emotion for Dataset by the load_dataset() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5706f64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: emotion/split\n",
      "Found cached dataset emotion (C:/Users/LENOVO/.cache/huggingface/datasets/emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf08e558ce84d47997c33165ae93a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a35814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions #look inside the emotion object"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4326203b",
   "metadata": {},
   "source": [
    "We see it is similar to the type Dictionary, with each key corresponding to a different split. And we can use the usual dictionary syntax to access an individual split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03dd0ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the value of key 'train'\n",
    "train_ds = emotions['train']\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d20bd9a2",
   "metadata": {},
   "source": [
    "It returns an instance of the Dataset class.The Dataset object is one of the core data structures in Datasets.We 'll be exploring many its features throughout the course of this book. Firstly, it appears like the list and array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9f68e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the length \n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f84afab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'i didnt feel humiliated', 'label': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "519edbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(train_ds[:5])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e427bec1",
   "metadata": {},
   "source": [
    "Here we see that a normal row is represented as a dictionary, where the keys correspond to the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d202b44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d264a64c",
   "metadata": {},
   "source": [
    "We can see the data types are being used under the hood by accsessing the features attribute of a Dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b734879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04126423",
   "metadata": {},
   "source": [
    "# From Datasets to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12334d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set_format() allows us to change the output format of the Dataset. \n",
    "# It does not change the underlying data format and we can switch to another format later\n",
    "\n",
    "emotions.set_format(type=\"pandas\")\n",
    "df = emotions[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a472a895",
   "metadata": {},
   "source": [
    "The labels are presented as integer, so let's use the int2str() method of labels feature to create a new column in out DataFrame with the corresponding label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "232f78e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_int2str(row):\n",
    "    return emotions[\"train\"].features[\"label\"].int2str(row)\n",
    "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecaeda",
   "metadata": {},
   "source": [
    "# Looking at the Class Distribution"
   ]
  },
  {
   "cell_type": "raw",
   "id": "714c17a3",
   "metadata": {},
   "source": [
    "Whenever you are working on text classification problems, it is a good idea to examine the distribution of examples across the classes. A dataset with a skewed class distribution might require a different treament in terms of the training loss and evaluation metrics than a balances one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23545a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "    df[\"label_name\"].value_counts(ascending = True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d0d36",
   "metadata": {},
   "source": [
    "# From Text to Tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "140c077e",
   "metadata": {},
   "source": [
    "Transformer models like DistillBERT cannot receive raw strings as input; instead,they assume the text has been tokenized and encoded as numerical vectors.\n",
    "Tokenization is the step of breaking down a string into the atomic units in the model. There are several tokenization strategies one can adopt, and the optimal splitting of words into subunits is usually learned from the corpus. Before looking aat the tokenizer used for DistillBERT, let's consider two extreme cases: character and word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ba2b7",
   "metadata": {},
   "source": [
    "# Character Tokeniztion"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fddaa9f",
   "metadata": {},
   "source": [
    "The simplest tokenization scheme is to feed each character individually to the model. In Python, \"str\" objects are really arrays under the hood, which allows us to quickly implement character-level tokenization with just one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f642244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Tokenization is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9c5631b",
   "metadata": {},
   "source": [
    "Our model expects each character to be concerted to an integer, a process sometimes called \"numericalization\". One simple way to do this is by encoding each unique token (which are characters in this case) with a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5be16cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'i': 10, 'k': 11, 'n': 12, 'o': 13, 'r': 14, 's': 15, 't': 16, 'z': 17}\n"
     ]
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx,ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba8f0f73",
   "metadata": {},
   "source": [
    "This gives us a mapping from each character in our vocabulary to a unique integer. We can now use \"token2idx\" to transform the tokenized text to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fd2f87e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 13, 11, 8, 12, 10, 17, 6, 16, 10, 13, 12, 0, 10, 15, 0, 6, 0, 7, 13, 14, 8, 0, 16, 6, 15, 11, 0, 13, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3566afe1",
   "metadata": {},
   "source": [
    "On the other hand, the resut of adding two one-hot encodings can easily be interpreted: the two entries that are \"hot\" indicate that the corresponding tokens co-ocur.We can create the one-hot encoding in PyTorch by converting \"input_ids\" to a tensor and applying the \"one_hot()\" function as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50aa04f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 18])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids,num_classes = len(token2idx))\n",
    "one_hot_encodings.shape\n",
    "\n",
    "# Hàm torch.tensor() được sử dụng để tạo một tensor mới. Tensor trong PyTorch\n",
    "# tương tự như mảng đa chiều trong NumPy nhưng có thể được sử dụng trên\n",
    "# GPU để tăng tốc độ tính toán.\n",
    "\n",
    "# Hàm one_hot() được sử dụng để biến đổi một tensor chứa các chỉ số\n",
    "# thành một biểu diễn one-hot."
   ]
  },
  {
   "cell_type": "raw",
   "id": "855af0d2",
   "metadata": {},
   "source": [
    "Kết quả là (35, 27). Số 35 ở đây đại diện cho số lượng phần tử trong tokenized_text, tức là số lượng ký tự trong chuỗi gốc của bạn.\n",
    "Số này bao gồm cả chữ cái, khoảng trắng và dấu chấm.\n",
    "\n",
    "Số 18 là num_classes là 18 dimensions, vì set của ta có 18 kí tự riêng biệt để biểu diễn chiều dài của 1 vector. Luôn phải có \"num_classses\" trong \"one_hot()\" vì nếu không vetor sẽ kết thúc ngắn hơn độ dài của từ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ce5785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token :T\n",
      "tensor index: 5\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Kiểm tra vector thứ nhất\n",
    "print(f\"Token :{tokenized_text[0]}\")\n",
    "print(f\"tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ae6160d",
   "metadata": {},
   "source": [
    " From our example above we can sê that character-level tokenization ignores any structure in the text and treats the whole string as a stream of characters. Although this helps deal with misspellings and rare words, the main drawback is that linguistic structures such as words need to be learned from the data. This requires significant compute, memory and data. For this reason, character tokenization is rearely used in practice. Instead, some structures of the text is preserved during the tokenization step. \"Word tokenizzation\" is a straightforward approach to achieve this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f0cec",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37e157e7",
   "metadata": {},
   "source": [
    "Instead of spliting the text into characters, we can split it into words and map each word to an integer. Using words from outset enables the model to skip the step of learning words from characters, and reduces the complexity of process\n",
    "One simple class of word tokenizers use white spaces to tokenize the text.We can do this by applying method \"split()\" function directly on the raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0a3dce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenizing', 'text', 'is', 'a', 'core', 'task', 'of', 'NLP.']\n"
     ]
    }
   ],
   "source": [
    "# Example:\n",
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b80faee",
   "metadata": {},
   "source": [
    "From here we can take the same steps we took for the character tokenizer to map each word to an ID. However we see one potention problem with this tokenization scheme: punctuation is not account for, so \"NLP.\" is treated as a single token\n",
    "     Some word tokenizers have extra rules for punctuation. One can also apply stemming or lemmatization,which normalizes word to their stem likes \"great\",\"Greater\" and \"greatest\" all become \"great\"\n",
    "     Having a large vocabulary is a problem because it requires neural networks to have an enormous number of parameters. To illustrate this, suppose we have 1 million unique words and want to compress the 1-million-dimensional input vectors to 1-thousand-dimensional vectors in the first layer of our neural network. This is a standard step in most NLP architectures, and the resulting weight matrix of this first layer would contain 1 million \n",
    " 1 thousand = 1 billion weights. This is already comparable to the largest GPT-2 model,footnote:[GPT-2 is the successor of GPT, and it captivated the public's attention with its impressive ability to generate realistic text. We'll explore GPT-2 in detail in <<chapter_summarization>>.] which has around 1.5 billion parameters in total!\n",
    " Naturally, we want to avoid being so wasteful with our model parameters since models are expensive to train, and larger models are more difficult to maintain.\n",
    " Wouldn't it be nice if there was a compromise between character and word tokenization that preserved all the input information and some of the input structure? There is: subword tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c055a",
   "metadata": {},
   "source": [
    "# Subword Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d814fcea",
   "metadata": {},
   "source": [
    "    The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization. On the one hand, we want to split rare words into smaller units to allow the model to deal with complex words and misspellings. On the other hand, we want to keep frequent words as unique entities so that we can keep the length of our inputs to a manageable size. The main distinguishing feature of subword tokenization (as well as word tokenization) is that it is learned from the pretraining corpus using a mix of statistical rules and algorithms.\n",
    "    There are several subword tokenization algorithms that are commonly used in NLP, but let's start with WordPiece, The easiest way to understand how WordPiece works is to see it in action. image:images/logo.png[hf,13,13] Transformers provides a convenient AutoTokenizer class that allows you to quickly load the tokenizer associated with a pretrained model—we just call its from_pretrained() method, providing the ID of a model on the Hub or a local file path. Let's start by loading the tokenizer for DistilBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "611e6475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# from transformers import DistilBertTokenizer\n",
    "# distil_bert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e58a4031",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ab5f1a4",
   "metadata": {},
   "source": [
    "    Just like we saw with character tokenization, we can see that the words have been mapped to unique integers in the \"input_ids\" field. We'll discuss the role of the \"attention_mask\" field in the next section. Now that we have the input_ids, we can convert them back into tokens by using the tokenizer's \"convert_ids_to_tokens()\" method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52e8691f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a60e9e2",
   "metadata": {},
   "source": [
    "    We can observe three things here. First, some special [CLS] and [SEP] tokens have been added to the start and end of the sequence. These tokens differ from model to model, but their main role is to indicate the start and end of a sequence. Second, the tokens have each been lowercased, which is a feature of this particular checkpoint. Finally, we can see that \"tokenizing\" and \"NLP\" have been split into two tokens, which makes sense since they are not common words. The \"##\" prefix in \"##izing\" and \"##p\" means that the preceding string is not whitespace; any token with this prefix should be merged with the previous token when you convert the tokens back to a string. The \"AutoTokenizer\" class has a \"convert_tokens_to_string()\" method for doing just that, so let's apply it to our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a73193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(token))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "def7e696",
   "metadata": {},
   "source": [
    "    The \"AutoTokenizer\" class also has several attributes that provide information about the tokenizer. For example, we can inspect the vocabulary size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044ef192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4e28dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09ffddc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another interesting attribute to know about is the names of the fields that the model \n",
    "# expects in its forward pass:\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68f442ec",
   "metadata": {},
   "source": [
    "    Warning: When using pretrained models, it is really important to make sure that you use the same tokenizer that the model was trained with. From the model's perspective, switching the tokenizer is like shuffling the vocabulary. If everyone around you started swapping random words like \"house\" for \"cat,\" you'd have a hard time understanding what was going on too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a154c91",
   "metadata": {},
   "source": [
    "# Tokenizing the Whole Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee67169d",
   "metadata": {},
   "source": [
    "    To tokenize the whole corpus, we'll use the map() method of our DatasetDict object. We'll encounter this method many times throughout this book, as it provides a convenient way to apply a processing function to each element in a dataset. As we'll soon see, the map() method can also be used to create new rows and columns.\n",
    "    To get started, the first thing we need is a processing function to tokenize our examples with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5b077b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],padding = True,truncation =True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b46719bf",
   "metadata": {},
   "source": [
    "    This function applies the tokenizer to a batch of examples; padding=True will pad the examples with zeros to the size of the longest one in a batch, and truncation=True will truncate the examples to the model's maximum context size. To see tokenize() in action, let's pass a batch of two examples from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ae8c249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "emotions.reset_format()\n",
    "print(tokenize(emotions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5def57f7",
   "metadata": {},
   "source": [
    "    Here we can see the result of padding: the first element of input_ids is shorter than the second, so zeros have been added to that element to make them the same length. These zeros have a corresponding [PAD] token in the vocabulary, and the set of special tokens also includes the [CLS] and [SEP] tokens that we encountered earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa376030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Special Token</th>\n",
       "      <td>[PAD]</td>\n",
       "      <td>[UNK]</td>\n",
       "      <td>[CLS]</td>\n",
       "      <td>[SEP]</td>\n",
       "      <td>[MASK]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Special Token ID</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>101</td>\n",
       "      <td>102</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0      1      2      3       4\n",
       "Special Token     [PAD]  [UNK]  [CLS]  [SEP]  [MASK]\n",
       "Special Token ID      0    100    101    102     103"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2ids = list(zip(tokenizer.all_special_tokens, tokenizer.all_special_ids))\n",
    "data = sorted(tokens2ids, key=lambda x : x[-1])\n",
    "df = pd.DataFrame(data, columns=[\"Special Token\", \"Special Token ID\"])\n",
    "df.T"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2458ca87",
   "metadata": {},
   "source": [
    "    Also note that in addition to returning the encoded tweets as input_ids, the tokenizer returns a list of attention_mask arrays. This is because we do not want the model to get confused by the additional padding tokens: the attention mask allows the model to ignore the padded parts of the input."
   ]
  },
  {
   "cell_type": "raw",
   "id": "04163108",
   "metadata": {},
   "source": [
    "    Once we've defined a processing function, we can apply it across all the splits in the corpus in a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb4580d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\LENOVO\\.cache\\huggingface\\datasets\\emotion\\split\\1.0.0\\cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd\\cache-b4245f74839e794e.arrow\n",
      "Loading cached processed dataset at C:\\Users\\LENOVO\\.cache\\huggingface\\datasets\\emotion\\split\\1.0.0\\cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd\\cache-d47b57812efeedb4.arrow\n",
      "Loading cached processed dataset at C:\\Users\\LENOVO\\.cache\\huggingface\\datasets\\emotion\\split\\1.0.0\\cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd\\cache-6c3cf07167669631.arrow\n"
     ]
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd0645be",
   "metadata": {},
   "source": [
    "    By default, the map() method operates individually on every example in the corpus, so setting batched=True will encode the tweets in batches. Because we've set batch_size=None, our tokenize() function will be applied on the full dataset as a single batch. This ensures that the input tensors and attention masks have the same shape globally, and we can see that this operation has added new input_ids and attention_mask columns to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d569841d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(emotions_encoded[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826da65c",
   "metadata": {},
   "source": [
    "# Fine-Tuning Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "98e7a152",
   "metadata": {},
   "source": [
    "    Let's now explore what it takes to fine-tune a transformer end-to-end. With the fine-tuning approach we do not use the hidden states as fixed features, but instead train them as shown in <>. This requires the classification head to be differentiable, which is why this method usually uses a neural network for classification."
   ]
  },
  {
   "cell_type": "raw",
   "id": "63f2464e",
   "metadata": {},
   "source": [
    "    Training the hidden states that serve as inputs to the classification model will help us avoid the problem of working with data that may not be well suited for the classification task. Instead, the initial hidden states adapt during training to decrease the model loss and thus increase its performance."
   ]
  },
  {
   "cell_type": "raw",
   "id": "98a0b194",
   "metadata": {},
   "source": [
    "    We'll be using the Trainer API from image:images/logo.png[hf,13,13] Transformers to simplify the training loop. Let's look at the ingredients we need to set one up!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c6db2",
   "metadata": {},
   "source": [
    "##  Loading a pretrained model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "554f5956",
   "metadata": {},
   "source": [
    "    The first thing we need is a pretrained DistilBERT model like the one we used in the feature-based approach. The only slight modification is that we use the AutoModelForSequenceClassification model instead of \"AutoModel\". The difference is that the AutoModelForSequenceClassification model has a classification head on top of the pretrained model outputs, which can be easily trained with the base model. We just need to specify how many labels the model has to predict (six in our case), since this dictates the number of outputs the classification head has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b5d347e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Jupyter\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "D:\\Jupyter\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8130ca",
   "metadata": {},
   "source": [
    "## Defining the performance metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2e6b0347",
   "metadata": {},
   "source": [
    "    To monitor metrics during training, we need to define a \"compute_metrics()\" function for the \"Trainer\". This function receives an \"EvalPrediction\" object (which is a named tuple with \"predictions\" and \"label_ids\" attributes) and needs to return a dictionary that maps each metric's name to its value. For our application, we'll compute the F1-score and the accuracy of the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d9c86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b673194",
   "metadata": {},
   "source": [
    "    With the dataset and metrics ready, we just have two final things to take care of before we define the \"Trainer\" class:\n",
    "    1.Log in to our account on the Hugging Face Hub. This will allow us to push our fine-tuned model to our account on the Hub and share it with the community.\n",
    "    2.Define all the hyperparameters for the training run.\n",
    "We'll tackle these steps in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413102aa",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c04e10a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2770a237102149b2839d215304e2d569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n",
    "     "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2faaa9eb",
   "metadata": {},
   "source": [
    "    To define the training parameters, we use the \"TrainingArguments\" class. This class stores a lot of information and gives you \"fine-grained\" control over the training and evaluation. The most important argument to specify is \"output_dir\", which is where all the artifacts from training are stored. Here is an example of \"TrainingArguments\" in all its glor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "457354b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "logging_steps = len(emotions_encoded[\"train\"])\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True, \n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4b6fb10",
   "metadata": {},
   "source": [
    "    Here we also set the batch size, learning rate, and number of epochs, and specify to load the best model at the end of the training run. With this final ingredient, we can instantiate and fine-tune our model with the Trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6505fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Jupyter\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 1:33:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.311660</td>\n",
       "      <td>0.906500</td>\n",
       "      <td>0.905517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.215584</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>0.922565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e1dccaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b1d1945",
   "metadata": {},
   "source": [
    "    The output of the \"predict()\" method is a \"PredictionOutput\" object that contains arrays of \"predictions\" and \"label_ids\", along with the metrics we passed to the trainer. For example, the metrics on the validation set can be accessed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f524bfe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.21558432281017303,\n",
       " 'test_accuracy': 0.923,\n",
       " 'test_f1': 0.9225647553629688,\n",
       " 'test_runtime': 60.3627,\n",
       " 'test_samples_per_second': 33.133,\n",
       " 'test_steps_per_second': 0.53}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40c5cbfe",
   "metadata": {},
   "source": [
    "    It also contains the raw predictions for each class. We can decode the predictions greedily using \"np.argmax()\". This yields the predicted labels and has the same format as the labels returned by the Scikit-Learn models in the feature-based approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1178c9d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (D:\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix\n\u001b[0;32m      4\u001b[0m y_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds_output\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m plot_confusion_matrix(y_preds, y_valid, labels)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (D:\\Jupyter\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03a30ee5",
   "metadata": {},
   "source": [
    "    Before moving on, we should investigate our model's predictions a little bit further. A simple yet powerful technique is to sort the validation samples by the model loss. When we pass the label during the forward pass, the loss is automatically calculated and returned. Here's a function that returns the loss along with the predicted label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8e714558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy\n",
    "\n",
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device), \n",
    "                             reduction=\"none\")\n",
    "\n",
    "    # Place outputs on CPU for compatibility with other dataset columns   \n",
    "    return {\"loss\": loss.cpu().numpy(), \n",
    "            \"predicted_label\": pred_label.cpu().numpy()}\n",
    "     "
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5c0f4af",
   "metadata": {},
   "source": [
    "    Using the map() method once more, we can apply this function to get the losses for all the samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7eec4547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emotions_encoded.set_format(\"torch\", \n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "402849be",
   "metadata": {},
   "source": [
    "    Finally, we create a DataFrame with the texts, losses, and predicted/true labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61f4ba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
    "                              .apply(label_int2str))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6986f665",
   "metadata": {},
   "source": [
    "    \n",
    "We can now easily sort emotions_encoded by the losses in either ascending or descending order. The goal of this exercise is to detect one of the following:\n",
    "\n",
    "Wrong labels:: Every process that adds labels to data can be flawed. Annotators can make mistakes or disagree, while labels that are inferred from other features can be wrong. If it was easy to automatically annotate data, then we would not need a model to do it. Thus, it is normal that there are some wrongly labeled examples. With this approach, we can quickly find and correct them.\n",
    "\n",
    "Quirks of the dataset:: Datasets in the real world are always a bit messy. When working with text, special characters or strings in the inputs can have a big impact on the model's predictions. Inspecting the model's weakest predictions can help identify such features, and cleaning the data or injecting similar examples can make the model more robust.\n",
    "\n",
    "Let's first have a look at the data samples with the highest losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b9fdc305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>i as representative of everything thats wrong ...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.712728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>i guess we would naturally feel a sense of lon...</td>\n",
       "      <td>anger</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.665506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>i would eventually go in to these stores but i...</td>\n",
       "      <td>joy</td>\n",
       "      <td>fear</td>\n",
       "      <td>5.127327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274</th>\n",
       "      <td>i am going to several holiday parties and i ca...</td>\n",
       "      <td>joy</td>\n",
       "      <td>sadness</td>\n",
       "      <td>5.094818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>i feel super awkward and out of place right now</td>\n",
       "      <td>joy</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4.941143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>i felt ashamed of these feelings and was scare...</td>\n",
       "      <td>fear</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4.808383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1801</th>\n",
       "      <td>i feel that he was being overshadowed by the s...</td>\n",
       "      <td>love</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4.792139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>i guess this is a memoir so it feels like that...</td>\n",
       "      <td>joy</td>\n",
       "      <td>fear</td>\n",
       "      <td>4.708754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>i feel badly about reneging on my commitment t...</td>\n",
       "      <td>love</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4.707018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>i guess i feel betrayed because i admired him ...</td>\n",
       "      <td>joy</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4.451698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label  \\\n",
       "1950  i as representative of everything thats wrong ...  surprise   \n",
       "1500  i guess we would naturally feel a sense of lon...     anger   \n",
       "465   i would eventually go in to these stores but i...       joy   \n",
       "1274  i am going to several holiday parties and i ca...       joy   \n",
       "765     i feel super awkward and out of place right now       joy   \n",
       "318   i felt ashamed of these feelings and was scare...      fear   \n",
       "1801  i feel that he was being overshadowed by the s...      love   \n",
       "1509  i guess this is a memoir so it feels like that...       joy   \n",
       "882   i feel badly about reneging on my commitment t...      love   \n",
       "1870  i guess i feel betrayed because i admired him ...       joy   \n",
       "\n",
       "     predicted_label      loss  \n",
       "1950         sadness  5.712728  \n",
       "1500         sadness  5.665506  \n",
       "465             fear  5.127327  \n",
       "1274         sadness  5.094818  \n",
       "765          sadness  4.941143  \n",
       "318          sadness  4.808383  \n",
       "1801         sadness  4.792139  \n",
       "1509            fear  4.708754  \n",
       "882          sadness  4.707018  \n",
       "1870         sadness  4.451698  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "19e87652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>i realize that i sound a little overdramatic w...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.015984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1140</th>\n",
       "      <td>i do think about certain people i feel a bit d...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.015985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>i and feel quite ungrateful for it but i m loo...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>i feel pathetic because i shouldn t complain a...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>im kinda relieve but at the same time i feel d...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>i feel quite jaded and unenthusiastic about li...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>i have no extra money im worried all of the ti...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>i leave the meeting feeling more than a little...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1452</th>\n",
       "      <td>i always feel guilty and come to one conclusio...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>i started this blog with pure intentions i mus...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.016456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label  \\\n",
       "571   i realize that i sound a little overdramatic w...  sadness   \n",
       "1140  i do think about certain people i feel a bit d...  sadness   \n",
       "133   i and feel quite ungrateful for it but i m loo...  sadness   \n",
       "1152  i feel pathetic because i shouldn t complain a...  sadness   \n",
       "244   im kinda relieve but at the same time i feel d...  sadness   \n",
       "866   i feel quite jaded and unenthusiastic about li...  sadness   \n",
       "69    i have no extra money im worried all of the ti...  sadness   \n",
       "189   i leave the meeting feeling more than a little...  sadness   \n",
       "1452  i always feel guilty and come to one conclusio...  sadness   \n",
       "1368  i started this blog with pure intentions i mus...  sadness   \n",
       "\n",
       "     predicted_label      loss  \n",
       "571          sadness  0.015984  \n",
       "1140         sadness  0.015985  \n",
       "133          sadness  0.016216  \n",
       "1152         sadness  0.016271  \n",
       "244          sadness  0.016300  \n",
       "866          sadness  0.016313  \n",
       "69           sadness  0.016393  \n",
       "189          sadness  0.016403  \n",
       "1452         sadness  0.016452  \n",
       "1368         sadness  0.016456  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "46f193a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/VuaCoBac/distilbert-base-uncased-finetuned-emotion/tree/main/'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "36e0f7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_id = \"transformersbook/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)\n",
    "custom_tweet = \"i have seen this movie\"\n",
    "preds = classifier(custom_tweet, return_all_scores=True)\n",
    "preds_df = pd.DataFrame(preds[0])\n",
    "labels = preds_df['label'].tolist()  # Lấy labels từ DataFrame\n",
    "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{custom_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
