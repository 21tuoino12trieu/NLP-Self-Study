{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697cefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cccd6bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Machine', 'Learning'),\n",
       " ('Learning', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'subset'),\n",
       " ('subset', 'of'),\n",
       " ('of', 'AI')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Machine Learning is a subset of AI\"\n",
    "ml_token = nltk.word_tokenize(text)\n",
    "list(nltk.bigrams(ml_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940dffa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1028a927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Machine', 'NN')]\n",
      "[('Learning', 'VBG')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('subset', 'NN')]\n",
      "[('of', 'IN')]\n",
      "[('AI', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for token in ml_token:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bff8969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Jerry', 'NN')]\n",
      "[(' ', 'NN')]\n",
      "[('eats', 'NNS')]\n",
      "[(' ', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[(' ', 'NN')]\n",
      "[('banana.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "sent = \"Jerry eats a banana.\"\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "reg_tokenizer = RegexpTokenizer('(?u)\\W+|\\$[\\d\\.]+|\\S+')\n",
    "tokens = reg_tokenizer.tokenize(sent)\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa17de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31654edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine', 'Learning', 'subset', 'AI']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data = [w for w in ml_token if not w in stop_words]\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04076c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine', 'Learning', 'is', 'a', 'subset', 'of', 'AI']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1732768",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fc7d6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous-->gener\n",
      "generate-->gener\n",
      "genorously-->genor\n",
      "generation-->gener\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "words = ['generous','generate','genorously','generation']\n",
    "for word in words:\n",
    "    print(f\"{word}-->{porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "141fc65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous-->generous\n",
      "generate-->generat\n",
      "genorously-->genor\n",
      "generation-->generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "words = ['generous','generate','genorously','generation']\n",
    "for word in words:\n",
    "    print(f\"{word}-->{snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d4c8a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous-->gen\n",
      "generate-->gen\n",
      "genorously-->gen\n",
      "generation-->gen\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster = LancasterStemmer()\n",
    "words = ['generous','generate','genorously','generation']\n",
    "for word in words:\n",
    "    print(f\"{word}-->{lancaster.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b58ee5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous-->generou\n",
      "generate-->generate\n",
      "genorously-->genorously\n",
      "generation-->generation\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg = RegexpStemmer('ing|s$|able$',min = 4)\n",
    "words = ['generous','generate','genorously','generation']\n",
    "for word in words:\n",
    "    print(f\"{word}-->{reg.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8974bbc",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8171bbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54caa8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "ran --> run\n"
     ]
    }
   ],
   "source": [
    "words = ['eating', 'ran']\n",
    "lemma = WordNetLemmatizer()\n",
    "for word in words:\n",
    "    if word.endswith('ing'):\n",
    "        print(f\"{word} --> {lemma.lemmatize(word, pos='v')}\")  \n",
    "    else:\n",
    "        print(f\"{word} --> {lemma.lemmatize(word,pos = 'v')}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c1cb28",
   "metadata": {},
   "source": [
    "# Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4eceb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('jamming.n.01') deliberate radiation or reflection of electromagnetic energy for the purpose of disrupting enemy use of electronic devices or systems\n",
      "Synset('jam.v.05') get stuck and immobilized\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "a1 = lesk(word_tokenize('The building has a device to jam the signal'),'jam')\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(word_tokenize('I am stuck in a traffic jam'),'jam')\n",
    "print(a2,a2.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "effbd3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('current.a.01') occurring in or belonging to the present time\n",
      "Synset('stream.n.02') dominant course (suggestive of running water) of successive events or ideas\n"
     ]
    }
   ],
   "source": [
    "a1  = lesk(word_tokenize('What is the current time ?'),'current')\n",
    "print(a1,a1.definition())\n",
    "a2 = lesk(word_tokenize('current air'),'current')\n",
    "print(a2,a2.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5192bf8",
   "metadata": {},
   "source": [
    "# Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49dc85ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f3f673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE Apple\n",
      "GPE American\n",
      "GPE California\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = 'Apple is an American company based out of California'\n",
    "for w in nltk.word_tokenize(text):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(w))): #Sử dụng nltk.ne_chunk để nhận diện named entities trong câu.\n",
    "        if hasattr(chunk,'label'):\n",
    "            print(chunk.label(),' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8ad333",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81e7d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\jupyter\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\jupyter\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\jupyter\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\jupyter\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\jupyter\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\jupyter\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\jupyter\\lib\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\jupyter\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\jupyter\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\jupyter\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in d:\\jupyter\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in d:\\jupyter\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\jupyter\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\jupyter\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\jupyter\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\jupyter\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in d:\\jupyter\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\jupyter\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\jupyter\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\jupyter\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\jupyter\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\jupyter\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\jupyter\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\jupyter\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in d:\\jupyter\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\jupyter\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in d:\\jupyter\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\jupyter\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7ca3546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 393.8 kB/s eta 0:00:33\n",
      "     --------------------------------------- 0.1/12.8 MB 819.2 kB/s eta 0:00:16\n",
      "      --------------------------------------- 0.2/12.8 MB 1.3 MB/s eta 0:00:11\n",
      "     - -------------------------------------- 0.3/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.5/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.8/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.1/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.5/12.8 MB 4.3 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.8/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 5.4 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.3/12.8 MB 5.9 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 6.7 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 7.1 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 6.2/12.8 MB 7.1 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 6.7/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.3/12.8 MB 8.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 10.8/12.8 MB 11.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.6/12.8 MB 12.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.4/12.8 MB 14.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 15.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 14.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in d:\\jupyter\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in d:\\jupyter\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in d:\\jupyter\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\jupyter\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\jupyter\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\jupyter\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in d:\\jupyter\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\jupyter\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in d:\\jupyter\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\jupyter\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c380da70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_context', '_get_array_attrs', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'from_json', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') #tagger,parser,NER\n",
    "text = \"\"\"Thirty days before the sale, we will publish a notice advertising bids. \n",
    "The notice will be in at least one local newspaper and in one trade publication. \n",
    "It will identify any reservation where the tracts to be leased are located.\"\"\"\n",
    "doc = nlp(text)\n",
    "print(dir(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dd91a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT            | LEMMA_          | POS_     | TAG_     | DEP_        | SHAPE_   | IS_ALPHA | IS_STOP  |\n",
      "She             | she             | PRON     | PRP      | nsubj       | Xxx      |        1 |        1 |\n",
      "is              | be              | AUX      | VBZ      | ROOT        | xx       |        1 |        1 |\n",
      "one             | one             | NUM      | CD       | attr        | xxx      |        1 |        1 |\n",
      "of              | of              | ADP      | IN       | prep        | xx       |        1 |        1 |\n",
      "the             | the             | DET      | DT       | det         | xxx      |        1 |        1 |\n",
      "best            | good            | ADJ      | JJS      | amod        | xxxx     |        1 |        0 |\n",
      "teacher         | teacher         | NOUN     | NN       | pobj        | xxxx     |        1 |        0 |\n",
      ".               | .               | PUNCT    | .        | punct       | .        |        0 |        0 |\n",
      "i               | I               | PRON     | PRP      | nsubj       | x        |        1 |        1 |\n",
      "loved           | love            | VERB     | VBD      | ROOT        | xxxx     |        1 |        0 |\n",
      "she             | she             | PRON     | PRP      | dobj        | xxx      |        1 |        1 |\n"
     ]
    }
   ],
   "source": [
    "text = 'She is one of the best teacher. i loved she'\n",
    "doc = nlp(text)\n",
    "\n",
    "#print column header\n",
    "print('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format('TEXT','LEMMA_','POS_','TAG_','DEP_','SHAPE_','IS_ALPHA','IS_STOP'))\n",
    "\n",
    "for token in doc:\n",
    "    print(('{:15} | {:15} | {:8} | {:8} | {:11} | {:8} | {:8} | {:8} |'.format(token.text,token.lemma_,token.pos_,token.tag_,token.dep_,token.shape_,token.is_alpha,token.is_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab741ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'determiner'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('det')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d39233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best teacher\n"
     ]
    }
   ],
   "source": [
    "previous_token = doc[0]\n",
    "\n",
    "for token in doc[1:]:\n",
    "    if previous_token.pos_ == 'ADJ' and token.pos_ == 'NOUN':\n",
    "        print(previous_token, token,sep = ' ')\n",
    "    previous_token = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "13d29fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL      | ENTITY                                            \n",
      "GPE        | VietNam                                           \n",
      "PERSON     | Machine Learning                                  \n"
     ]
    }
   ],
   "source": [
    "ner_text = 'I love my country VietNam, and i love Machine Learning'\n",
    "\n",
    "ner_tok = nlp(ner_text)\n",
    "print('{:10} | {:50}'.format('LABEL','ENTITY'))\n",
    "for ent in ner_tok.ents:\n",
    "    print('{:10} | {:50}'.format(ent.label_,ent.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ecf41834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I love my country \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    VietNam\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and i love \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Machine Learning\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(docs=ner_tok,style = 'ent',jupyter = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
