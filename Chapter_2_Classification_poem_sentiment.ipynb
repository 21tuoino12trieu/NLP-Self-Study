{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a96e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset poem_sentiment (C:/Users/LENOVO/.cache/huggingface/datasets/poem_sentiment/default/1.0.0/4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ca2350a40b4e0393a35adf2a37d9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "poem = load_dataset(\"poem_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3445abbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 892\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 105\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'verse_text', 'label'],\n",
       "        num_rows: 104\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ebb1163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>verse_text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>with pale blue berries. in these peaceful shad...</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>it flows so long as falls the rain,</td>\n",
       "      <td>2</td>\n",
       "      <td>no_impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>and that is why, the lonesome day,</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>when i peruse the conquered fame of heroes, an...</td>\n",
       "      <td>3</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>of inward strife for truth and liberty.</td>\n",
       "      <td>3</td>\n",
       "      <td>mixed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                         verse_text  label label_name\n",
       "0   0  with pale blue berries. in these peaceful shad...      1   positive\n",
       "1   1                it flows so long as falls the rain,      2  no_impact\n",
       "2   2                 and that is why, the lonesome day,      0   negative\n",
       "3   3  when i peruse the conquered fame of heroes, an...      3      mixed\n",
       "4   4            of inward strife for truth and liberty.      3      mixed"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def label_int2str(row):\n",
    "    return poem['train'].features['label'].int2str(row)\n",
    "poem.set_format(type = \"pandas\")\n",
    "df = poem['train'][:]\n",
    "df['label_name'] = df['label'].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55eb7be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d01baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['verse_text'],padding =True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa59d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\LENOVO\\.cache\\huggingface\\datasets\\poem_sentiment\\default\\1.0.0\\4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099\\cache-f5a2c3bcf6cb3658.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\LENOVO\\.cache\\huggingface\\datasets\\poem_sentiment\\default\\1.0.0\\4e44428256d42cdde0be6b3db1baa587195e91847adabf976e4f9454f6a82099\\cache-53e95a9d39ca3d99.arrow\n"
     ]
    }
   ],
   "source": [
    "poem.reset_format()\n",
    "poem_encoded = poem.map(tokenize,batched = True,batch_size = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9662c5e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [0, 1, 2, 3, 4], 'verse_text': ['with pale blue berries. in these peaceful shades--', 'it flows so long as falls the rain,', 'and that is why, the lonesome day,', 'when i peruse the conquered fame of heroes, and the victories of mighty generals, i do not envy the generals,', 'of inward strife for truth and liberty.'], 'label': [1, 2, 0, 3, 3], 'input_ids': [[101, 2007, 5122, 2630, 22681, 1012, 1999, 2122, 9379, 13178, 1011, 1011, 102], [101, 2009, 6223, 2061, 2146, 2004, 4212, 1996, 4542, 1010, 102], [101, 1998, 2008, 2003, 2339, 1010, 1996, 10459, 14045, 2154, 1010, 102], [101, 2043, 1045, 7304, 3366, 1996, 11438, 4476, 1997, 7348, 1010, 1998, 1996, 9248, 1997, 10478, 11593, 1010, 1045, 2079, 2025, 21103, 1996, 11593, 1010, 102], [101, 1997, 20546, 27865, 2005, 3606, 1998, 7044, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(poem_encoded['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98adb71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Jupyter\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "D:\\Jupyter\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "num_labels = 4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\", 3: \"mixed\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "model = (AutoModelForSequenceClassification\n",
    "         .from_pretrained(model_ckpt, num_labels=num_labels,id2label=id2label,\n",
    "    label2id=label2id)\n",
    "         .to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce72ca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75328bf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f99e500c7cb4cb99754588a008667cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7a6a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "logging_steps = len(poem_encoded[\"train\"])\n",
    "model_name = f\"{model_ckpt}-finetuned-rating-poem\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"steps\",  # Đánh giá sau mỗi X bước\n",
    "    eval_steps=50,  # Số bước mỗi lần đánh giá\n",
    "    save_strategy=\"steps\",  # Lưu sau mỗi X bước\n",
    "     logging_steps=50,\n",
    "    save_steps=50,  # Số bước mỗi lần lưu\n",
    "    save_total_limit=1,  # Số lượng checkpoints tối đa để giữ lại\n",
    "     adam_beta1=0.9,                  # beta1 cho thuật toán Adam\n",
    "    adam_beta2=0.999,                # beta2 cho thuật toán Adam\n",
    "    adam_epsilon=1e-8,                # epsilon cho thuật toán Adam\n",
    "    load_best_model_at_end=True,  # Tải mô hình tốt nhất ở cuối quá trình\n",
    "    metric_for_best_model=\"f1\",  # Sử dụng F1 để đánh giá mô hình tốt nhất\n",
    "    greater_is_better=True,  # Đặt là True nếu giá trị F1 càng cao càng tốt\n",
    "    push_to_hub=False,  # Nếu bạn muốn đẩy mô hình của mình lên Hugging Face Hub\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f61072b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Jupyter\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n",
      "The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 892\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 560\n",
      "  Number of trainable parameters = 66,956,548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 03:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.059900</td>\n",
       "      <td>1.024689</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.861061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>1.123651</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.849957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>1.134650</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.856658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>1.218105</td>\n",
       "      <td>0.838095</td>\n",
       "      <td>0.837252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>1.042279</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.866699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>1.056027</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.871507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.012800</td>\n",
       "      <td>1.115573</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.880903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>1.170158</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.868087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>1.196843</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.867750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>1.190613</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.876510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>1.190161</td>\n",
       "      <td>0.876190</td>\n",
       "      <td>0.876510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-50\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-50\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-50\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-50\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-50\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-100\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-100\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-100\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-100\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-100\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-150\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-150\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-150\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-150\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-150\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-100] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-200\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-200\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-200\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-150] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-250\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-250\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-250\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-250\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-250\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-50] due to args.save_total_limit\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-200] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-300\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-300\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-300\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-300\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-300\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-250] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-350\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-350\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-350\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-350\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-350\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-300] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-400\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-400\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-400\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-450\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-450\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-450\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-450\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-500\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-500\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-500\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-450] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\\checkpoint-550\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-550\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-550\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-550\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\checkpoint-550\\special_tokens_map.json\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from distilbert-base-uncased-finetuned-rating-poem\\checkpoint-350 (score: 0.8809033747809258).\n",
      "Deleting older checkpoint [distilbert-base-uncased-finetuned-rating-poem\\checkpoint-550] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=poem_encoded[\"train\"],\n",
    "                  eval_dataset = poem_encoded['validation'],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "365c82f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: id, verse_text. If id, verse_text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 105\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_output = trainer.predict(poem_encoded[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "286f5ea5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 1.1155728101730347,\n",
       " 'test_accuracy': 0.8857142857142857,\n",
       " 'test_f1': 0.8809033747809258,\n",
       " 'test_runtime': 0.6743,\n",
       " 'test_samples_per_second': 155.706,\n",
       " 'test_steps_per_second': 20.761}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26d3b6eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-base-uncased-finetuned-rating-poem\n",
      "Configuration saved in distilbert-base-uncased-finetuned-rating-poem\\config.json\n",
      "Model weights saved in distilbert-base-uncased-finetuned-rating-poem\\pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-base-uncased-finetuned-rating-poem\\tokenizer_config.json\n",
      "Special tokens file saved in distilbert-base-uncased-finetuned-rating-poem\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43baea553e20480e9ee4607f2403c635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d130883a6a47fc92c063b9d163dc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e6c59af88441e69a0759312959aa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/VuaCoBac/distilbert-base-uncased-finetuned-rating-poem/tree/main/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
